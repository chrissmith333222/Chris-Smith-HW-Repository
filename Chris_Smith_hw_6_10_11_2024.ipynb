{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac5bcfb-020e-44b4-a0f1-7a8ca177fa1b",
   "metadata": {},
   "source": [
    "# HW 6\n",
    "\n",
    "1. Translating Sentences:\n",
    "10 pts\n",
    "Write a function that translates sentences. The function takes two arguements, the sentence to be translated and tthe targeted language and returns the translated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089667a-5110-4b19-9c09-71c2fb8f8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "#ChatGPT\n",
    "\n",
    "#installing google translator on my jupyter notebook\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "\n",
    "#I am importing the google translate library for use in the function\n",
    "from googletrans import Translator\n",
    "\n",
    "#creating a dictionary to map language names to language codes so that I can just type in a language name without using a code.  \n",
    "language_map = {\n",
    "    'english': 'en',\n",
    "    'spanish': 'es',\n",
    "    'french': 'fr',\n",
    "    'german': 'de',\n",
    "    'italian': 'it',\n",
    "    'chinese': 'zh-cn',\n",
    "    'japanese': 'ja',\n",
    "    'russian': 'ru',\n",
    "    'arabic': 'ar',\n",
    "    'portuguese': 'pt',\n",
    "}\n",
    "#creating the function for translate_text to take inputs based on the language selected and then translate them \n",
    "def translate_text(text, target_language_name='english'):\n",
    "    #translator function is below\n",
    "    translator = Translator()\n",
    "    #defining the targte language code by pulling from the langauge map library\n",
    "    target_language_code = language_map.get(target_language_name.lower(), 'en')  # Default to English if not found\n",
    "    #defining the translation variable by use of the translator function\n",
    "    translation = translator.translate(text, dest=target_language_code)\n",
    "    return translation.text\n",
    "\n",
    "# Create an input where people input the text that they want to translate\n",
    "text_to_translate = input(\"Enter the text to translate: \")\n",
    "#create an input where people select the laguage that they want the words translated to\n",
    "language = input(\"Enter the target language (e.g., 'French', 'Spanish', 'German'): \")\n",
    "\n",
    "#running the function based on teh inputs\n",
    "translated_text = translate_text(text_to_translate, language)\n",
    "#prin the results of the function\n",
    "print(f\"Translated text: {translated_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da605bc1-1bdb-43e3-aede-d73912faf915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acb9b1ca-0b7a-4367-b0d4-0eec50b068d8",
   "metadata": {},
   "source": [
    "Take the following sentence and translate it from English to Spanish.\n",
    "\"Make a career of humanity. Commit yourself to the noble struggle for equal rights. You will make a better person of yourself, a greater nation of your country, and a finer world to live in.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa7388-cea8-4a12-a589-192a57688461",
   "metadata": {},
   "source": [
    "#used the code from my above work and ran it based on the inputs\n",
    "\n",
    "Hacer una carrera de la humanidad.Comprométate con la noble lucha por la igualdad de derechos.Harás una mejor persona de ti mismo, una nación más grande de tu país y un mundo más fino para vivir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0373e-686c-4372-a49c-a583620161c3",
   "metadata": {},
   "source": [
    "2. Now translate it from Spanish to Chinese\n",
    "2.5pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb2685-2420-4b5a-b3ae-967b2daf5fbd",
   "metadata": {},
   "source": [
    "\n",
    "#used the code above and ran it using the inputs from the first question: \"Hacer una carrera de la humanidad.Comprométate con la noble lucha por la igualdad de derechos.Harás una mejor persona de ti mismo, una nación más grande de tu país y un mundo más fino para vivir.\"\n",
    "\n",
    "# 通过为平等权利而努力的人类职业。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f2361-5bd0-4bcb-9dc4-7ee6630cbabd",
   "metadata": {},
   "source": [
    "3. Now translate it from Chinese back to English.\n",
    "2.5pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdb436-6dfc-4783-94f8-fc7a696cce63",
   "metadata": {},
   "source": [
    "\n",
    "#I redid the same process as before using the chinese output and reinserted it back into the same translator, this time asking for English\n",
    "\n",
    "#translated text: \"Human occupations that work hard for equal rights\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22fb28-eb8c-4394-b291-22f695d04469",
   "metadata": {},
   "source": [
    "4. Now do the same thing with ChatGPT.\n",
    "\n",
    "2.5pts\n",
    "Eng to Sp: Sp to Chinese: Chinese to Eng:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe41485-3a9f-4e8f-94ac-69650e816203",
   "metadata": {},
   "source": [
    "ChatGPT English to Spanish: \"Haz una carrera de la humanidad. Comprométete con la noble lucha por los derechos iguales. Harás de ti una mejor persona, de tu país una nación más grande y del mundo un lugar mejor para vivir.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77ecd6-b1b8-4a0d-9741-52d1675bef42",
   "metadata": {},
   "source": [
    "ChatGPT Spanish to Chinese: \"以人道为事业。致力于为平等权利而进行的崇高斗争。你会使自己成为一个更好的人，使你的国家成为一个更伟大的国家，使世界成为一个更美好的生活之地。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4272a-7b9a-4c6c-8e06-4ee6462c64ad",
   "metadata": {},
   "source": [
    "ChatGPT Chinese to English: \"Make humanity your career. Commit yourself to the noble struggle for equal rights. You will make yourself a better person, your country a greater nation, and the world a better place to live in.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f22fc5-75b5-4e6e-8c2e-00291f468035",
   "metadata": {},
   "source": [
    "5. Describe the difference in quality and features of output:\n",
    "2.5pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9ee3c-4b51-4382-b8c0-2d5add161985",
   "metadata": {},
   "source": [
    "ChatGPT does a much better job than the translator in maintaining the true meaning and sense of the sentence.  I believe that this is because the translator does not rely on context and the ChatGPT transformer trains itself and can back reference the previous inputs and prompts that you have given it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f45014-5aa0-4fab-b48f-a468d1261f06",
   "metadata": {},
   "source": [
    "Your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf5ad4-25ec-4eef-bc6a-bd480e64b65a",
   "metadata": {},
   "source": [
    " ## 7.  The Trump Card\n",
    "Here is a link to a speech made by former President Trump on January 6: https://www.npr.org/2021/02/10/966396848/read-trumps-jan-6-speech-a-key-part-of-impeachment-trial. Not all of the text is his speech. Some of the text is the analysis. Please extract Donald Trump's speech only, remove stopwords and perform a word frequency distribution and visualize it.\n",
    "20pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe20edf-d767-482e-b835-7dbdbbf5684c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31ab15-ca68-4a6b-852e-deda45033790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for web scraping, natural language processing (NLP), and visualization.  This is the same beautifulsoap, nltk, and pyplot as last class\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download necessary resources from NLTK (tokenizer and stopwords list)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Set up headers for the request to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Step 2: Fetch the webpage content using the 'requests' library\n",
    "base_url = \"https://www.npr.org/2021/02/10/966396848/read-trumps-jan-6-speech-a-key-part-of-impeachment-trial\"\n",
    "try:\n",
    "    r = requests.get(base_url, headers=headers)\n",
    "    r.raise_for_status()  # Check if the request was successful\n",
    "except requests.exceptions.RequestException as e:\n",
    "    #print error message if needed\n",
    "    print(f\"Error fetching content from {base_url}: {e}\")\n",
    "else:\n",
    "    # Step 3: Parse the webpage content using BeautifulSoup\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Step 4: Find all paragraph elements ('<p>') in the HTML\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    if paragraphs:\n",
    "        # Step 5: Extract paragraphs starting from the 6th one (index 5).  This is how I isolate the portion of the page that is just Trump's Speech, since his speech starts with paragraph 6 on the web page.\n",
    "        trump_document = paragraphs[5:]  # Slicing the list to get paragraphs from index 5 onwards\n",
    "\n",
    "        # Step 6: Create an empty list to store the speech text\n",
    "        speech_text_list = []\n",
    "\n",
    "        # Step 7: Iterate over the remaining paragraphs and extract text from each\n",
    "        for p in trump_document:\n",
    "            text = p.get_text()  # Get the text content of the paragraph\n",
    "            speech_text_list.append(text)  # Append the text to the list\n",
    "\n",
    "        # Combine all paragraphs into one large string of text for analysis\n",
    "        speech_text = ' '.join(speech_text_list)\n",
    "\n",
    "        print(\"Speech successfully extracted from the NPR webpage.\")\n",
    "    else:\n",
    "        print(\"No paragraphs found on the page.\")\n",
    "\n",
    "# Step 3: Load a set of common stopwords (like \"the\", \"and\") from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 4: Define a function to clean the text by removing stopwords and non-alphabetic characters\n",
    "def clean_text(text, stop_words):\n",
    "    # Break the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Keep only alphabetic words (ignore numbers and symbols) and remove stopwords\n",
    "    clean_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Return the frequency distribution of the cleaned words\n",
    "    return FreqDist(clean_words)\n",
    "\n",
    "# Step 5: Process the speech text to remove stopwords and calculate word frequencies\n",
    "fdist = clean_text(speech_text, stop_words)\n",
    "\n",
    "# Step 6: Define a function to plot the frequency distribution of words\n",
    "def plot_fdist(fdist, title=\"Word Frequency Distribution\"):\n",
    "    # Get the 20 most common words and their frequencies\n",
    "    most_common_words = fdist.most_common(20)\n",
    "    \n",
    "    # Separate the words and their counts for plotting\n",
    "    words = [word for word, freq in most_common_words]\n",
    "    frequencies = [freq for word, freq in most_common_words]\n",
    "    \n",
    "    # Create a bar chart using matplotlib\n",
    "    plt.figure(figsize=(10, 6))  # Set the size of the plot\n",
    "    plt.bar(words, frequencies)  # Create the bar chart with words on x-axis and frequencies on y-axis\n",
    "    plt.xticks(rotation=45)  # Rotate the words so they fit better\n",
    "    plt.title(title)  # Set the title of the chart\n",
    "    plt.xlabel(\"Words\")  # Label the x-axis\n",
    "    plt.ylabel(\"Frequency\")  # Label the y-axis\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "# Call the visualization function to plot the word frequency distribution of Trump's speech\n",
    "plot_fdist(fdist, \"Donald Trump's Speech Word Frequency Distribution\")\n",
    "\n",
    "# Step 7: Define a function to save the frequency distribution to a text file\n",
    "def save_fdist_to_file(fdist, filename):\n",
    "    # Open the file in write mode\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write the top 20 most frequent words and their counts to the file\n",
    "        for word, frequency in fdist.most_common(20):\n",
    "            f.write(f'{word}: {frequency}\\n')  # Format: word: frequency\n",
    "\n",
    "# Save the word frequencies to a file called 'word_frequencies.txt'\n",
    "save_fdist_to_file(fdist, 'word_frequencies.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b87a8-223c-4dc0-a838-523871fb3a75",
   "metadata": {},
   "source": [
    "8. Here is a link to a github repo that contains Donald Trump's speeches: https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt\n",
    "   \n",
    "What are the 10 most common things Donald Trump \"loves?\"\n",
    "10pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b97a7-618d-4fc2-969d-f72cdb7fd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT\n",
    "\n",
    "# Import libraries we need for making web requests, processing text, and showing charts.  These are based on porevious class work\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # This helps us get the text from a webpage\n",
    "import nltk  # NLTK is a library for working with text (natural language processing)\n",
    "from nltk.probability import FreqDist  # This helps count how often words appear\n",
    "from collections import Counter  # This helps count things like words or phrases\n",
    "import matplotlib.pyplot as plt  # This is used for making charts\n",
    "import re  # Regular expressions (regex) allow us to find patterns in text\n",
    "\n",
    "# Download resources from NLTK (the tokenization tool that splits text into words)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function that fetches the speech text from a GitHub URL\n",
    "def fetch_speech_from_github(url):\n",
    "    try:\n",
    "        # Try to send a request to the URL and get the content of the webpage\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()  # Check if the request was successful (status code 200)\n",
    "        return r.text  # Return the text content of the webpage (the speech text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If something went wrong (like a bad URL), print an error message\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return None  # Return None if there was an error\n",
    "\n",
    "# URL where the speech text is stored on GitHub \n",
    "github_speech_url = 'https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt'\n",
    "\n",
    "# get the speech text from GitHub by calling the function we defined earlier\n",
    "speech_text = fetch_speech_from_github(github_speech_url)\n",
    "\n",
    "# Check if we successfully retrieved the speech text\n",
    "if speech_text:\n",
    "    # Tokenize (split) the speech into words\n",
    "    tokens = nltk.word_tokenize(speech_text)\n",
    "\n",
    "    # Step 1: Use a regular expression to find all instances of \"I love\" followed by 1 to 5 words\n",
    "    # The pattern finds \"I love\" followed by 1 to 5 words (\\w+ matches any word, {1,5} means 1 to 5 words)\n",
    "    pattern = re.compile(r'\\bi love\\b(?:\\s+\\w+){1,5}', re.IGNORECASE)\n",
    "    matches = pattern.findall(speech_text)  # Find all matches of this pattern in the speech text\n",
    "\n",
    "    # Step 2: Prepare to store single words and multi-word phrases separately\n",
    "    single_words = []  # This will hold single words that come after \"I love\"\n",
    "    multi_word_phrases = []  # This will hold multi-word phrases (more than 1 word) after \"I love\"\n",
    "\n",
    "    # Loop over each \"I love\" match we found\n",
    "    for match in matches:\n",
    "        # Split the match into individual words\n",
    "        words = match.split()\n",
    "        # If there are only 3 words (i.e., \"I love [single_word]\"), treat it as a single word\n",
    "        if len(words) == 3:\n",
    "            single_words.append(' '.join(words[2:]))  # Take the word after \"I love\" and add to single_words list\n",
    "        else:\n",
    "            # If more than 3 words, treat it as a phrase (more than 1 word after \"I love\")\n",
    "            multi_word_phrases.append(' '.join(words[2:]))  # Take all words after \"I love\" and add to multi_word_phrases list\n",
    "\n",
    "    # Step 3: Count how often each single word and each phrase appears\n",
    "    single_word_counter = Counter(single_words)  # Count each single word that follows \"I love\"\n",
    "    multi_word_counter = Counter(multi_word_phrases)  # Count each multi-word phrase that follows \"I love\"\n",
    "\n",
    "    # Step 4: Combine the counts of single words and multi-word phrases\n",
    "    combined_counter = single_word_counter + multi_word_counter  # Add the two counters together\n",
    "    most_common_combined = combined_counter.most_common(10)  # Get the 10 most common words/phrases\n",
    "\n",
    "    # Step 5: Print the top 10 most common words/phrases after \"I love\"\n",
    "    print(\"Top words/phrases that follow 'I love':\")\n",
    "    for phrase, count in most_common_combined:\n",
    "        print(f\"'{phrase}': {count}\")  # Print each phrase and how often it appears\n",
    "\n",
    "    # Step 6: Prepare to visualize the top 10 words/phrases with a bar chart\n",
    "    phrases, frequencies = zip(*most_common_combined)  # Separate phrases and their frequencies into two lists\n",
    "    \n",
    "    # Create a bar chart using matplotlib to show the top words/phrases\n",
    "    plt.figure(figsize=(10, 6))  # Set the size of the chart\n",
    "    plt.barh(phrases, frequencies)  # Create a horizontal bar chart\n",
    "    plt.title(\"Top 10 Words/Phrases Following 'I love'\")  # Title of the chart\n",
    "    plt.xlabel(\"Frequency\")  # Label for the x-axis (number of times each phrase appears)\n",
    "    plt.ylabel(\"Words/Phrases\")  # Label for the y-axis (the actual words/phrases)\n",
    "    plt.show()  # Display the chart\n",
    "\n",
    "else:\n",
    "    # If we couldn't get the speech text, print this message\n",
    "    print(\"Failed to retrieve the speech text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07782a4-2931-4e23-bb33-b9fab29182ca",
   "metadata": {},
   "source": [
    "9. Which are the top 5 countries Trump mentions in his speeches, besides America or the United States of America?\n",
    "10pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb639d1-6f9e-444c-8248-75b74bf569c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code and answer here:\n",
    "#ChatGPT\n",
    "\n",
    "# Import libraries needed to fetch web content, process text, and display graphs\n",
    "import requests\n",
    "import nltk  # NLTK is used for splitting text into words\n",
    "from collections import Counter  # Counter helps count occurrences of items\n",
    "import matplotlib.pyplot as plt  # Used to create graphs (visualization)\n",
    "import pycountry  # Provides a list of country names\n",
    "import re  # Used for working with regular expressions (patterns in text)\n",
    "\n",
    "# Download necessary resources from NLTK (tokenizer)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to fetch text content from a GitHub repository URL (e.g., where speeches are stored)\n",
    "def get_speech_text_from_github(github_url):\n",
    "    try:\n",
    "        # Send a request to fetch the content from the GitHub URL\n",
    "        response = requests.get(github_url)\n",
    "        response.raise_for_status()  # Check if the request was successful (status 200)\n",
    "        return response.text  # Return the text content from the webpage\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        # Print an error message if something goes wrong\n",
    "        print(f\"Error fetching content from {github_url}: {error}\")\n",
    "        return None  # Return None if the content can't be fetched\n",
    "\n",
    "# URL where the speech text is stored on GitHub \n",
    "speech_github_url = 'https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt'\n",
    "\n",
    "# Fetch the speech text from the GitHub URL\n",
    "speech_content = get_speech_text_from_github(speech_github_url)\n",
    "\n",
    "# Get a list of all country names using the pycountry library\n",
    "country_list = [country.name for country in pycountry.countries]\n",
    "\n",
    "# Function to identify country names in the speech text\n",
    "def find_countries_in_text(speech_text):\n",
    "    # Tokenize (split) the speech into individual words\n",
    "    speech_words = nltk.word_tokenize(speech_text)\n",
    "    \n",
    "    # Create an empty list to store the countries that are mentioned\n",
    "    mentioned_countries = []\n",
    "    \n",
    "    # Loop through each word in the tokenized speech\n",
    "    for word in speech_words:\n",
    "        # Check if the word matches any country name from the list of countries\n",
    "        if word in country_list:\n",
    "            mentioned_countries.append(word)  # Add the country to the list if found\n",
    "    \n",
    "    return mentioned_countries  # Return the list of countries found in the speech\n",
    "\n",
    "# If the speech content was successfully retrieved\n",
    "if speech_content:\n",
    "    # Step 1: Find the countries that are mentioned in the speech\n",
    "    countries_in_speech = find_countries_in_text(speech_content)\n",
    "    \n",
    "    # Step 2: Count how often each country is mentioned\n",
    "    country_mention_counter = Counter(countries_in_speech)\n",
    "    \n",
    "    # Step 3: Get the top 5 most mentioned countries from the speech\n",
    "    top_5_countries = country_mention_counter.most_common(5)\n",
    "    \n",
    "    # Step 4: Print the top 5 countries and the number of times they are mentioned\n",
    "    print(\"Top 5 countries mentioned in Trump's speech:\")\n",
    "    for country, count in top_5_countries:\n",
    "        print(f\"'{country}': {count}\")\n",
    "    \n",
    "    # Step 5: Create a bar chart to visualize the top 5 countries mentioned\n",
    "    country_names, mention_frequencies = zip(*top_5_countries)  # Unzip the country names and their counts\n",
    "    \n",
    "    # Set the size of the bar chart figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a horizontal bar chart with the country names and their mention frequencies\n",
    "    plt.barh(country_names, mention_frequencies)\n",
    "    \n",
    "    # Add a title and labels to the chart\n",
    "    plt.title(\"Top 5 Countries Mentioned in Trump's Speeches\")\n",
    "    plt.xlabel(\"Frequency\")  # Label for the x-axis (how many times the country is mentioned)\n",
    "    plt.ylabel(\"Country\")  # Label for the y-axis (country names)\n",
    "    \n",
    "    # Show the bar chart\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Print a message if the speech content couldn't be retrieved\n",
    "    print(\"Failed to retrieve the speech text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025cea9c-b4d6-47a4-8910-826c521d594b",
   "metadata": {},
   "source": [
    "\n",
    " In class we talked about tokenizing sentences into words, or parsing texts by single words(unigrams), or two words(bigrams). But you can also tokenize by sentences. Here is some sample code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9af68c-a8e1-4d1e-ab49-e791d071bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is the first sentence. This is the second sentence.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481e654-f253-4573-948b-0cf63d2b22c7",
   "metadata": {},
   "source": [
    "In class we talked about final projects and I suggested one of the interesting uses of this technology is to consider conventional thinking on some subject and use Python to more comprehensively and critically evaluate that thinking. We touched on \"political thinking\" as a good place to apply Python language analyses. In political speeches, certain sentences or phrases are repeated to show a candidates coaching, focus or in marketing terms, \"messaging.\" \n",
    "\n",
    "Here is an analyis from https://www.vox.com/2016/8/18/12423688/donald-trump-speech-style-explained-by-linguists of the \"salesman\" techniques that Trump uses: \"Trump’s speeches can be appealing because he uses a lot of salesmen’s tricks. Lakoff, for his part, has an explanation for why Trump’s style of speaking is so appealing to many. Many of Trump’s most famous catchphrases are actually versions of time-tested speech mechanisms that salesmen use. They’re powerful because they help shape our unconscious. Take, for example, Trump’s frequent use of \"Many people are saying...\" or \"Believe me\" — often right after saying something that is baseless or untrue. This tends to sound more trustworthy to listeners than just outright stating the baseless claim, since Trump implies that he has direct experience with what he’s talking about. At a base level, Lakoff argues, people are more inclined to believe something that seems to have been shared. Or when Trump keeps calling Clinton \"crooked,\" or keeps referring to terrorists as \"radical Muslims,\" he’s strengthening the association through repetition. He also calls his supporters \"folks,\" to show he is one of them (though many politicians employ this trick). Trump doesn’t repeat phrases and adjectives because he is stalling for time, Liberman says; for the most part, he’s providing emphasis and strengthening the association. These are normal techniques, particularly in conversational speech. \"Is he reading cognitive science? No. He has 50 years of experience as a salesman who doesn’t care who he is selling to,\" Lakoff says. On this account, Trump uses similar methods in his QVC-style pitch of steaks and vodka as when he talks about his plan to stop ISIS.\"He has been doing this for a very long time as a salesman — that’s what he is best at,\" Lakoff says.\"  \n",
    "*This is not to say I agree or disagree with this analysis.*  This is just one example of what we touched on in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e1cb4-90b1-4f3e-9948-6ec6935401ea",
   "metadata": {},
   "source": [
    "10. Perform a frequency analysis that provides evidence for or against the assertion made in the Vox article.\n",
    "Consider tokenizing by unigram (one word), bigram(two words), trigram(three words) or more, or whole sentences or multiple approaches that help us understand the most common Trump linguistic characteristcs. Use your evidence and words to describe what you found. This is a fairly open ask. Don't just execute code. Tell me and show me something interesting!\n",
    "20pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68933a0b-53da-465c-99f3-5c4bf1a3212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT\n",
    "# Import necessary libraries for web scraping, text processing, and visualization\n",
    "import requests\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Download necessary resources from NLTK (tokenizer for words)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to fetch the speech text from the GitHub URL\n",
    "def fetch_speech_text_from_github(url):\n",
    "    try:\n",
    "        # Send a request to the GitHub URL and get the text content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure the request was successful (status 200)\n",
    "        return response.text  # Return the raw text content from the webpage\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return None  # Return None if there's an error\n",
    "\n",
    "# URL of Trump's speeches from GitHub that we'll use to check his word frequency\n",
    "speech_github_url = 'https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt'\n",
    "\n",
    "# Fetch the speech text from GitHub\n",
    "speech_text = fetch_speech_text_from_github(speech_github_url)\n",
    "\n",
    "# List of salesman-like phrases and words to search for in his speeches\n",
    "search_words_phrases = [\n",
    "    'biggest', 'smallest', 'greatest', 'worst', 'incredible', \n",
    "    'horrendous', 'terrible', 'amazing', 'least', 'most', \n",
    "    'fastest', 'slowest', 'strongest', 'weakest', 'believe me', \n",
    "    'everyone knows', 'many people are saying',\n",
    "]\n",
    "\n",
    "# Function to count the occurrences of specific words and multi-word phrases in the speech text\n",
    "def count_phrases_in_speech(text, words_to_search):\n",
    "    # Initialize a counter\n",
    "    phrase_counter = Counter()\n",
    "    \n",
    "    # Normalize text to lowercase for case-insensitive matching\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Loop through each word or phrase in the search list\n",
    "    for phrase in words_to_search:\n",
    "        # Use regular expressions to find exact matches of phrases\n",
    "        phrase_count = len(re.findall(r'\\b' + re.escape(phrase) + r'\\b', text))\n",
    "        if phrase_count > 0:\n",
    "            phrase_counter[phrase] += phrase_count\n",
    "    \n",
    "    return phrase_counter\n",
    "\n",
    "# If the speech text was successfully retrieved\n",
    "if speech_text:\n",
    "    # Count the occurrences of the specific words and phrases\n",
    "    word_frequencies = count_phrases_in_speech(speech_text, search_words_phrases)\n",
    "    \n",
    "    # Print the frequency of each specific word/phrase\n",
    "    print(\"\\nWord frequencies in Trump's speeches (specific words/phrases):\")\n",
    "    for phrase, count in word_frequencies.items():\n",
    "        print(f\"'{phrase}': {count}\")\n",
    "    \n",
    "    # Visualize the frequency of the specific words and phrases in a bar chart\n",
    "    words, frequencies = zip(*word_frequencies.items())  # Unzip the words/phrases and their frequencies\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.barh(words, frequencies, color='lightcoral')  # Create a horizontal bar chart\n",
    "    plt.title(\"Frequency of Specific Words and Phrases in Trump's Speeches\")  # Title of the chart\n",
    "    plt.xlabel(\"Frequency\")  # Label for the x-axis\n",
    "    plt.ylabel(\"Words/Phrases\")  # Label for the y-axis\n",
    "    plt.show()  # Display the chart\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the speech text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf979e84-7015-4b94-a07a-80ace5f5914a",
   "metadata": {},
   "source": [
    "To assess the claim, I looked at the frequecy that Trump uses superlative language as well as a few key phrases identified in the article.  I chose this because salesman often use superlatives and that can be an indicator of using sales strategies in political speeches.  In comparing them to the previous distributions such as coutnry names, and 20 most common words, we see that trump uses words like \"incredible\", \"amazing\", and \"most\" more frequently than most other words.  China was one of the few words that Trump used more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0bd617-57e6-4f8f-bf44-3aba3ebb671c",
   "metadata": {},
   "source": [
    "## 6. Mired in Mango Madness ##\n",
    "20pts\n",
    "\n",
    "Congratulations! In your last semester at Yale, you got a job at the World Bank ! \n",
    "\n",
    "Your first assignment is to address the dearth of good data in Mali on Mango quality and production. Your boss put you on a plane to Mali during the harvest season. You are excited, not only because your boss is annoying and wears too much cologne, but international travel is part of the excitement of an international job. You are excited to see the Great Mosque of Djenne, to watch the sunset from the Bandiagara Cliffs, and to dance the night away to  at Le Byblos. Oh yeah, there is the work too. After a particular \"rough\" night you meet with your colleagues at the office.  You thought you'd be able to just get by making eye contact and nodding while they did most of the work. After all, you are new to the role. No such luck. Your colleagues are thrilled that you can solve their problems by creating an automated call center. Their exact words are, \"The Harvard grad ended up quitting, I'm glad we have someone from Yale here to make things right.\"\n",
    "\n",
    "You are given an office and a computer. You may have slightly overstated your Python coding skills to get the job, but too late now. They ask if you mind if they watch you code. You tell them that you prefer to work alone, although ask them to lunch later to soften the push back. After the initial panic and hangover subsides you think back to your days in intermediate python at Yale, the encouraging words of your caring but demanding prof,  and then a light flashes in your mind and you think, \"I got this.\" \n",
    "\n",
    "You've been tasked with the goal of setting up an automated call center to get data on current market prices, production and quality. There are growers who are both collectives who report these factors, and also smaller growers! Unfortunately, some of these are incentivized to underreport production and prices under the assumption that they will receive more foreign aid. Not all of them though! It's estimated that 10-20% of the self-reporting is innacurate. Callers call the center. Words are processed from speech to text. Text is processed, converted back to speech and then communicated to the caller. \n",
    "\n",
    "There are a few challenges you face: \n",
    "1. The speech to text algo was trained on French, but most of the farmers who call in speak Bambara. You must translate from Bambara to French, and then French to English, because your boss doesn't speak French and wants things in English and is completely unreasonable.\n",
    "2. Sometimes the call just ends, sometimes words are sporadic, sometimes the algo. confuses the word \"child\" and the word \"mango\"\n",
    "3. The call in number was once the number for the infant health hotline. People still use the number to try to get immediate emergency care for their infant. Their calls must be redirected or children could die and it would be your fault, not to mention an international incident.\n",
    "4. The call number is often called by fans and musicians who are given the number by directory assistance because there is a music production company called Mango Mali, a fledgling world music group that markets music from Mali to France.\n",
    "5. You must try to identify who is lying about their crop yield quality and prices.\n",
    "\n",
    "\n",
    "**YOUR MISSION**\n",
    "\n",
    "Write code that accepts the number as an input and then collects user input(in Bambara), translates it to French, and then outputs the data to a csv file in English. Please make sure you write code that addresses the challenges above (1-5). \n",
    "Create use cases to demonstrate that your code works and that you have addressed the basic use case, and that you have addressed challenges 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beef55f1-a398-4d34-9647-bf77a6735ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: translate in c:\\users\\chris\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\chris\\anaconda3\\lib\\site-packages (from translate) (8.1.7)\n",
      "Requirement already satisfied: lxml in c:\\users\\chris\\anaconda3\\lib\\site-packages (from translate) (5.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\lib\\site-packages (from translate) (2.32.2)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click->translate) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->translate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->translate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->translate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->translate) (2024.7.4)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your 10-digit phone number:  2153533357\n",
      "Are you a 'farming collective' or 'individual farmer'? (in Bambara):  farming collective\n",
      "What is your charged price per kilogram of mangoes in USD? (in Bambara):  1.34\n",
      "What is the quality of your mangoes: 'excellent', 'good', 'fair', or 'poor'? (in Bambara):  excellent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to French: agriculture collective. 1.34. excellent.\n",
      "Translated to English: collective agriculture. 1.34. excellent.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many mangos did you produce?  890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Farming collectives typically produce more than 1000 mangos. Flagging for underreporting.\n",
      "Warning: Quality reported as 'excellent'. Flagging for review.\n",
      "Thank you for your input!\n",
      "\n",
      "Data saved to 'call_center_data.csv'\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT\n",
    "#Import necessary libraries for text processing, translation, and CSV output\n",
    "!pip install translate\n",
    "\n",
    "from translate import Translator  \n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Initialize the translators for Bambara to French and French to English\n",
    "translator_bm_to_fr = Translator(from_lang=\"bm\", to_lang=\"fr\")  # Bambara to French\n",
    "translator_fr_to_en = Translator(from_lang=\"fr\", to_lang=\"en\")  # French to English\n",
    "\n",
    "# Create an empty DataFrame to store the results (for saving to CSV later)\n",
    "data_df = pd.DataFrame(columns=['Phone Number', 'Caller Type', 'Bambara Input', 'English Output', 'Mango Production', 'Price per kg', 'Quality', 'Market Prices', 'Production'])\n",
    "\n",
    "# Define keywords that may indicate the user mistakenly called emergency services or a music service\n",
    "emergency_keywords = ['child', 'infant', 'baby', 'hospital', 'help']\n",
    "music_keywords = ['mango mali', 'music', 'song', 'album', 'artist']\n",
    "\n",
    "# Helper function to translate Bambara text to English via French\n",
    "def translate_bambara_to_english(bambara_text):\n",
    "    # Translate from Bambara to French\n",
    "    translated_to_french = translator_bm_to_fr.translate(bambara_text)\n",
    "    print(f\"Translated to French: {translated_to_french}\")\n",
    "    \n",
    "    # Then, translate from French to English\n",
    "    translated_to_english = translator_fr_to_en.translate(translated_to_french)\n",
    "    print(f\"Translated to English: {translated_to_english}\")\n",
    "    \n",
    "    # Return the final English translation\n",
    "    return translated_to_english\n",
    "\n",
    "# Function to detect if the input contains emergency or music-related keywords\n",
    "def detect_special_case(text):\n",
    "    # Check for any emergency-related keywords in the input\n",
    "    if any(keyword in text.lower() for keyword in emergency_keywords):\n",
    "        return 'Emergency'\n",
    "    # Check for any music-related keywords in the input\n",
    "    if any(keyword in text.lower() for keyword in music_keywords):\n",
    "        return 'Music Call'\n",
    "    # If no special keywords detected, return None\n",
    "    return None\n",
    "\n",
    "# Function to handle the call\n",
    "def handle_call():\n",
    "    # Step 1: Prompt the user to enter their phone number\n",
    "    phone_number = input(\"Please enter your 10-digit phone number: \")\n",
    "    # Validate if the entered number is 10 digits long, if not, return an error and stop\n",
    "    if not re.match(r'^\\d{10}$', phone_number):\n",
    "        print(\"Invalid phone number. Please enter a valid 10-digit number.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Ask the first question in Bambara: Grower Type (farming collective or individual farmer)\n",
    "    grower_type_bambara = input(\"Are you a 'farming collective' or 'individual farmer'? (in Bambara): \")\n",
    "    \n",
    "    # Check if the input contains emergency or music-related content before proceeding\n",
    "    special_case = detect_special_case(grower_type_bambara)\n",
    "    if special_case:\n",
    "        # If an emergency or music-related keyword is detected, print the special case message and stop further questions\n",
    "        print(f\"Redirecting to appropriate service: {special_case}\")\n",
    "        # Log the data with 'special_case' status in the CSV\n",
    "        data_df.loc[len(data_df)] = [phone_number, special_case, grower_type_bambara, '', '', '', '', '', '']\n",
    "        return  # Stop here if special case detected\n",
    "\n",
    "    # Step 3: Ask the second question in Bambara: Price per kilogram of mangoes in USD\n",
    "    price_bambara = input(\"What is your charged price per kilogram of mangoes in USD? (in Bambara): \")\n",
    "    \n",
    "    # Check again if any emergency or music-related keywords are entered in the price input\n",
    "    special_case = detect_special_case(price_bambara)\n",
    "    if special_case:\n",
    "        # If detected, log and stop further questions\n",
    "        print(f\"Redirecting to appropriate service: {special_case}\")\n",
    "        data_df.loc[len(data_df)] = [phone_number, special_case, price_bambara, '', '', '', '', '', '']\n",
    "        return\n",
    "\n",
    "    # Step 4: Ask the third question in Bambara: Quality of the mangoes (excellent, good, fair, or poor)\n",
    "    quality_bambara = input(\"What is the quality of your mangoes: 'excellent', 'good', 'fair', or 'poor'? (in Bambara): \")\n",
    "    \n",
    "    # Final check for emergency or music keywords in the quality input\n",
    "    special_case = detect_special_case(quality_bambara)\n",
    "    if special_case:\n",
    "        # If detected, log and stop further questions\n",
    "        print(f\"Redirecting to appropriate service: {special_case}\")\n",
    "        data_df.loc[len(data_df)] = [phone_number, special_case, quality_bambara, '', '', '', '', '', '']\n",
    "        return\n",
    "\n",
    "    # Combine all the Bambara inputs for translation and processing\n",
    "    bambara_text = f\"{grower_type_bambara}. {price_bambara}. {quality_bambara}.\"\n",
    "\n",
    "    # Step 5: Translate the combined Bambara text to English via French\n",
    "    english_text = translate_bambara_to_english(bambara_text)\n",
    "\n",
    "    # Step 6: Extract information from the English translation\n",
    "\n",
    "    # Identify grower type\n",
    "    grower_type = 'farming collective' if 'collective' in english_text.lower() else 'individual farmer'\n",
    "    \n",
    "    # Extract price per kilogram from the translated text using regex to find numbers\n",
    "    try:\n",
    "        price_per_kg = float(re.search(r\"\\d+(\\.\\d+)?\", english_text).group())  # Extract price per kg from the text\n",
    "    except AttributeError:\n",
    "        # If no valid price is found, display an error and stop\n",
    "        print(\"Could not extract a valid price per kilogram.\")\n",
    "        return\n",
    "\n",
    "    # Determine mango quality based on translated text\n",
    "    quality = 'excellent' if 'excellent' in english_text.lower() else 'good' if 'good' in english_text.lower() else 'fair' if 'fair' in english_text.lower() else 'poor'\n",
    "\n",
    "    # Step 7: Ask for mango production (this part is in English)\n",
    "    mango_production = int(input(\"How many mangos did you produce? \"))\n",
    "\n",
    "    # Step 8: Determine if the report should be flagged for any special conditions\n",
    "    fraud_status = \"Legitimate Report\"  # Default status\n",
    "\n",
    "    # Flag underreporting if grower is a farming collective and reports <= 1000 mangos\n",
    "    if grower_type == \"farming collective\" and mango_production <= 1000:\n",
    "        print(\"Warning: Farming collectives typically produce more than 1000 mangos. Flagging for underreporting.\")\n",
    "        fraud_status = \"Underreporting Suspected\"\n",
    "\n",
    "    # Flag low price if price per kilogram is below $1.00\n",
    "    if price_per_kg < 1.00:\n",
    "        print(\"Warning: Price per kilogram is below $1.00. Flagging for review.\")\n",
    "        fraud_status = \"Low Price Flagged\"\n",
    "\n",
    "    # Flag if the quality is reported as \"excellent\"\n",
    "    if quality == \"excellent\":\n",
    "        print(\"Warning: Quality reported as 'excellent'. Flagging for review.\")\n",
    "        fraud_status = \"Quality Flagged\"\n",
    "\n",
    "    # Step 9: Save the data to the DataFrame\n",
    "    data_df.loc[len(data_df)] = [phone_number, fraud_status, bambara_text, english_text, mango_production, price_per_kg, quality, 'N/A', 'N/A']\n",
    "\n",
    "    # Final message to thank the user for their input\n",
    "    print(\"Thank you for your input!\")\n",
    "\n",
    "# Call the function to simulate one call\n",
    "handle_call()\n",
    "\n",
    "# Save the collected data to a CSV file\n",
    "data_df.to_csv(r'C:\\Users\\chris\\OneDrive\\Desktop\\Yale\\Fall 2024\\GLBL 5050\\Homework\\call_center_data.csv', index=False)\n",
    "print(\"\\nData saved to 'call_center_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1619e-b5c9-4463-b14d-cf700aa0b425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822c3cb-63f1-4b10-8468-a612cf17b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
